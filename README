Question 1 - Bad XML: 
Create an RDD of Post objects where each Post is a valid row of XML from
the Cross-Validated (stats.stackexchange.com) allPosts data set.

Return the total number of XML rows that started with <row that were subsequently 
rejected during your processing. 

** Note that the text is unicode, and contains non-ASCII characters. 
You may need to re-encode to UTF-8 (depending on your XML parser)


Question 2 - Favorites and Scores:
Look to see if there is a relationship between the number of times the post was 
upvoted minus the number of times it was downvoted, so it is a measure of 
how much a post was liked. We'd expect posts with higher number of favorites to 
have better scores, since they're both measurements of how good the post is.

Let's aggregate post by the number of favorites, and find the average score for 
each number of favorites. Do this for the lowest 50 number of favorites.


Question 3 - Answer percentage:
Investigate the correlation between a user's reputation and the kind of posts 
they make. For the 99 users with the highest reputation, single out posts which 
are either questions or answers and look at the percentage of these posts that 
are answers: (answers / (answers + questions)).

Return a tuple of their user ID and this fraction.

You should also return (-1, fraction) to represent the case where you average 
over all users (so you will return 100 entries total).


Question 4 - First Question: 
We'd expect the first question a user asks to be indicative of their future 
behavior. We'll dig more into that in the next problem, but for now let's see 
the relationship between reputation and how long it took each person to ask 
their first question.

For each user that asked a question, find the difference between when their 
account was created (CreationDate for the User) and when they asked their 
first question (CreationDate for their first question). Return this time 
difference in days (round down, so 2.7 days counts as 2 days) for the 
100 users with the highest reputation, in the form (UserId, Days).

Question 5 - Identify Veterans:
It can be interesting to think about what factors influence a user to remain 
active on the site over a long period of time. In order not to bias the results 
towards older users, we'll define a time window between 100 and 150 days after 
account creation. If the user has made a post in this time, we'll consider them
active and well on their way to being veterans of the site; if not, they are 
inactive and were likely brief users.

Let's see if there are differences between the first ever question posts of 
"veterans" vs. "brief users". For each group separately, average the score, 
views, number of answers, and number of favorites of the users' first question.


Question 6 - Identify Veterans Full
Same as question 5, but will use the full Stack Exchange data set. 


Question 7 - Word2vec
Let's see how good a Word2Vec model we can train using the tags of each Stack 
Exchange post as documents (this uses the full data set). Use the implementation 
of Word2Vec from Spark ML (this will require using DataFrames) to return a list 
of the top 25 closest synonyms to "ggplot2" and their similarity score in tuple 
format ("string", number).

Question 8 - Classification
We'd like to see if we can predict the tags of a question from its body text. 
Instead of predicting specific tags, we will instead try to predict if a 
question contains one of the top ten most common tags.

To this end, we have separated out a train and a test set from the original data. 
The training and tests sets were downloaded with the stats data at the beginning 
of the notebook. 

Return a list of your predictions, sorted by the question's Id. 
This sorting is very important, as our grader expects the results to be 
submitted in a particular order. These predictions should be 0 if the question 
isn't expected to have a tag in the top ten, and 1 if it is.

